{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNV6tlBSIF6E"
      },
      "source": [
        " **==================== MODEL DEFINITIONS ====================**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH5-iz-uIOrb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRC3avK_HsA8"
      },
      "source": [
        "# **CoverageLayer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKq3RDwpHiOc"
      },
      "outputs": [],
      "source": [
        "class CoverageLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CoverageLayer, self).__init__()\n",
        "        self.register_buffer('activations', None)\n",
        "        self.num_sections = 10\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.activations = x.detach()\n",
        "        return x\n",
        "\n",
        "    def get_neuron_outputs(self):\n",
        "        return self.activations\n",
        "\n",
        "    def compute_section_distribution(self, min_val, max_val):\n",
        "        activations = self.activations\n",
        "        delta = (max_val - min_val) / (self.num_sections - 1)\n",
        "        delta = torch.where(delta == 0, torch.tensor(1e-10, device=delta.device), delta)\n",
        "\n",
        "        section_indices = torch.clamp(((activations - min_val) / delta).long(), 0, self.num_sections - 1)\n",
        "        section_counts = torch.bincount(section_indices.view(-1), minlength=self.num_sections)\n",
        "        section_distributions = section_counts.float() / activations.numel()\n",
        "        return section_distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QLjqJkXH2F8"
      },
      "source": [
        "# **VGG19WithCoverage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrYo1Lo1HiMP"
      },
      "outputs": [],
      "source": [
        "class VGG19WithCoverage(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG19WithCoverage, self).__init__()\n",
        "        original_vgg = models.vgg19(pretrained=True)\n",
        "\n",
        "        # Feature layers\n",
        "        self.conv1_1 = original_vgg.features[0]\n",
        "        self.conv1_2 = original_vgg.features[2]\n",
        "        self.pool1 = original_vgg.features[4]\n",
        "\n",
        "        self.conv2_1 = original_vgg.features[5]\n",
        "        self.conv2_2 = original_vgg.features[7]\n",
        "        self.pool2 = original_vgg.features[9]\n",
        "\n",
        "        self.conv3_1 = original_vgg.features[10]\n",
        "        self.conv3_2 = original_vgg.features[12]\n",
        "        self.conv3_3 = original_vgg.features[14]\n",
        "        self.conv3_4 = original_vgg.features[16]\n",
        "        self.pool3 = original_vgg.features[18]\n",
        "\n",
        "        self.conv4_1 = original_vgg.features[19]\n",
        "        self.conv4_2 = original_vgg.features[21]\n",
        "        self.conv4_3 = original_vgg.features[23]\n",
        "        self.conv4_4 = original_vgg.features[25]\n",
        "        self.pool4 = original_vgg.features[27]\n",
        "\n",
        "        self.conv5_1 = original_vgg.features[28]\n",
        "        self.conv5_2 = original_vgg.features[30]\n",
        "        self.conv5_3 = original_vgg.features[32]\n",
        "        self.conv5_4 = original_vgg.features[34]\n",
        "        self.pool5 = original_vgg.features[36]\n",
        "\n",
        "        # Classifier layers\n",
        "        self.fc1 = original_vgg.classifier[0]\n",
        "        self.fc2 = original_vgg.classifier[3]\n",
        "        self.fc3 = original_vgg.classifier[6]\n",
        "\n",
        "        # Coverage layers\n",
        "        self.coverage_layers = nn.ModuleList([CoverageLayer() for _ in range(18)])\n",
        "        self.avgpool = original_vgg.avgpool\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = F.relu(self.conv1_1(x))\n",
        "        x = self.coverage_layers[0](x)\n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.coverage_layers[1](x)\n",
        "\n",
        "        # Block 2\n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = self.coverage_layers[2](x)\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.coverage_layers[3](x)\n",
        "\n",
        "        # Block 3\n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = self.coverage_layers[4](x)\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = self.coverage_layers[5](x)\n",
        "        x = F.relu(self.conv3_3(x))\n",
        "        x = self.coverage_layers[6](x)\n",
        "        x = F.relu(self.conv3_4(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.coverage_layers[7](x)\n",
        "\n",
        "        # Block 4\n",
        "        x = F.relu(self.conv4_1(x))\n",
        "        x = self.coverage_layers[8](x)\n",
        "        x = F.relu(self.conv4_2(x))\n",
        "        x = self.coverage_layers[9](x)\n",
        "        x = F.relu(self.conv4_3(x))\n",
        "        x = self.coverage_layers[10](x)\n",
        "        x = F.relu(self.conv4_4(x))\n",
        "        x = self.pool4(x)\n",
        "        x = self.coverage_layers[11](x)\n",
        "\n",
        "        # Block 5\n",
        "        x = F.relu(self.conv5_1(x))\n",
        "        x = self.coverage_layers[12](x)\n",
        "        x = F.relu(self.conv5_2(x))\n",
        "        x = self.coverage_layers[13](x)\n",
        "        x = F.relu(self.conv5_3(x))\n",
        "        x = self.coverage_layers[14](x)\n",
        "        x = F.relu(self.conv5_4(x))\n",
        "        x = self.pool5(x)\n",
        "        x = self.coverage_layers[15](x)\n",
        "\n",
        "        # Classifier\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.coverage_layers[16](x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.coverage_layers[17](x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPtkTK6mLRsk"
      },
      "source": [
        "# **==================== DATA LOGGING ====================**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-OBGwBVLPNc"
      },
      "outputs": [],
      "source": [
        "class TrainingDataLogger:\n",
        "    def __init__(self):\n",
        "        self.data = {\n",
        "            'metadata': {\n",
        "                'start_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'end_time': None,\n",
        "                'total_epochs': 0\n",
        "            },\n",
        "            'epochs': [],\n",
        "            'config': {}\n",
        "        }\n",
        "\n",
        "    def add_epoch_data(self, epoch, train_metrics, test_metrics, coverage_stats):\n",
        "        epoch_data = {\n",
        "            'epoch': epoch,\n",
        "            'train': train_metrics,\n",
        "            'test': {\n",
        "                'trusted': test_metrics['trusted'],\n",
        "                'untrusted': test_metrics['untrusted']\n",
        "            },\n",
        "            'coverage': {}\n",
        "        }\n",
        "\n",
        "        # Add coverage layer statistics\n",
        "        for i, layer_stats in enumerate(test_metrics['trusted']['coverage_stats']):\n",
        "            epoch_data['coverage'][f'layer_{i+1}'] = {\n",
        "                'trusted_coverage': layer_stats['coverage_percentage'],\n",
        "                'untrusted_coverage': test_metrics['untrusted']['coverage_stats'][i]['coverage_percentage'],\n",
        "                'distribution_similarity': layer_stats['distribution_similarity']\n",
        "            }\n",
        "\n",
        "        self.data['epochs'].append(epoch_data)\n",
        "\n",
        "    def finalize(self, total_epochs):\n",
        "        self.data['metadata']['end_time'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        self.data['metadata']['total_epochs'] = total_epochs\n",
        "\n",
        "    def save(self, filename):\n",
        "        # Save as JSON (human-readable)\n",
        "        json_filename = f\"{filename}.json\"\n",
        "        with open(json_filename, 'w') as f:\n",
        "            json.dump(self.data, f, indent=4)\n",
        "\n",
        "        # Save as pickle (preserves Python objects)\n",
        "        pkl_filename = f\"{filename}.pkl\"\n",
        "        with open(pkl_filename, 'wb') as f:\n",
        "            pickle.dump(self.data, f)\n",
        "\n",
        "        print(f\"Saved training data to {json_filename} and {pkl_filename}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load(filename):\n",
        "        with open(f\"{filename}.pkl\", 'rb') as f:\n",
        "            return pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtCLdsVZIVe-"
      },
      "source": [
        "**==================== TRAINING COMPONENTS ====================**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrF793fXIwCw"
      },
      "source": [
        "# **SignatureGenerator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM45kwDjHiKB"
      },
      "outputs": [],
      "source": [
        "class SignatureGenerator:\n",
        "    def __init__(self, model, coverage_layer_indices, method):\n",
        "        self.model = model\n",
        "        self.coverage_layer_indices = coverage_layer_indices\n",
        "        self.method = method\n",
        "\n",
        "    def aggregate_signatures(self, trusted_loader, device):\n",
        "        self.model.eval()\n",
        "        signatures = [None] * len(self.model.coverage_layers)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Initialize min/max with first batch\n",
        "            data, _ = next(iter(trusted_loader))\n",
        "            data = data.to(device)\n",
        "            self.model(data)\n",
        "\n",
        "            for i in self.coverage_layer_indices:\n",
        "                layer = self.model.coverage_layers[i]\n",
        "                activations = layer.get_neuron_outputs()\n",
        "                signatures[i] = {\n",
        "                    'min': activations.min(dim=0)[0],\n",
        "                    'max': activations.max(dim=0)[0],\n",
        "                    'distributions': None\n",
        "                }\n",
        "\n",
        "            # Process remaining batches\n",
        "            for images, _ in trusted_loader:\n",
        "                images = images.to(device)\n",
        "                self.model(images)\n",
        "\n",
        "                for i in self.coverage_layer_indices:\n",
        "                    layer = self.model.coverage_layers[i]\n",
        "                    activations = layer.get_neuron_outputs()\n",
        "\n",
        "                    current_min = activations.min(dim=0)[0]\n",
        "                    current_max = activations.max(dim=0)[0]\n",
        "\n",
        "                    signatures[i]['min'] = torch.min(signatures[i]['min'], current_min)\n",
        "                    signatures[i]['max'] = torch.max(signatures[i]['max'], current_max)\n",
        "\n",
        "            if self.method == 'mrc':\n",
        "                for i in self.coverage_layer_indices:\n",
        "                    layer = self.model.coverage_layers[i]\n",
        "                    signatures[i]['distributions'] = layer.compute_section_distribution(\n",
        "                        signatures[i]['min'],\n",
        "                        signatures[i]['max']\n",
        "                    )\n",
        "\n",
        "        return signatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvFLBToEI7HR"
      },
      "source": [
        "# **ConfidenceLoss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs3-P1aTHiH4"
      },
      "outputs": [],
      "source": [
        "class ConfidenceLoss(nn.Module):\n",
        "    def __init__(self, signatures, coverage_layer_indices, method):\n",
        "        super(ConfidenceLoss, self).__init__()\n",
        "        self.signatures = signatures\n",
        "        self.coverage_layer_indices = coverage_layer_indices\n",
        "        self.method = method\n",
        "\n",
        "        # Pre-compute and cache values\n",
        "        self.cached_min_vals = []\n",
        "        self.cached_max_vals = []\n",
        "        self.cached_distributions = []\n",
        "\n",
        "        for i in coverage_layer_indices:\n",
        "            self.cached_min_vals.append(signatures[i]['min'])\n",
        "            self.cached_max_vals.append(signatures[i]['max'])\n",
        "            self.cached_distributions.append(signatures[i]['distributions'] if method == 'mrc' else None)\n",
        "\n",
        "    def forward(self, model):\n",
        "        total_loss = torch.tensor(0.0, device=next(model.parameters()).device)\n",
        "\n",
        "        for idx, layer_idx in enumerate(self.coverage_layer_indices):\n",
        "            layer = model.coverage_layers[layer_idx]\n",
        "            activations = layer.get_neuron_outputs()\n",
        "\n",
        "            min_vals = self.cached_min_vals[idx].to(activations.device)\n",
        "            max_vals = self.cached_max_vals[idx].to(activations.device)\n",
        "\n",
        "            if self.method == 'src':\n",
        "                outside_range = torch.logical_or(activations < min_vals, activations > max_vals).float().mean()\n",
        "                total_loss += outside_range\n",
        "            elif self.method == 'mrc':\n",
        "                outside_range = torch.logical_or(activations < min_vals, activations > max_vals).float().mean()\n",
        "\n",
        "                num_sections = model.coverage_layers[layer_idx].num_sections\n",
        "                delta = (max_vals - min_vals) / (num_sections - 1)\n",
        "                delta = torch.where(delta == 0, torch.tensor(1e-10, device=delta.device), delta)\n",
        "\n",
        "                section_indices = torch.clamp(((activations - min_vals) / delta).long(), 0, num_sections - 1)\n",
        "                current_dist = torch.bincount(section_indices.view(-1), minlength=num_sections).float()\n",
        "                current_dist /= activations.numel()\n",
        "\n",
        "                signature_dist = self.cached_distributions[idx].to(activations.device)\n",
        "                distribution_loss = F.mse_loss(current_dist, signature_dist)\n",
        "\n",
        "                total_loss += outside_range + distribution_loss\n",
        "\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WopbGHq3JVyy"
      },
      "source": [
        "**==================== TRAINING AND TESTING ====================**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSYDz_YEJaaE"
      },
      "source": [
        "# **train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brHAFSyyHiCW"
      },
      "outputs": [],
      "source": [
        "# ==================== TRAINING AND TESTING ====================\n",
        "def train(model, device, train_loader, optimizer, epoch, confidence_loss_fn, signatures, coverage_layer_indices, confidence_threshold=0.8):\n",
        "    model.train()\n",
        "    total_loss = total_confidence_loss = total_ce_loss = 0.0\n",
        "    correct = high_confidence_correct = total_samples = high_confidence_total = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        ce_loss = F.cross_entropy(output, target)\n",
        "        conf_loss = confidence_loss_fn(model)\n",
        "        loss = ce_loss + conf_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate metrics\n",
        "        with torch.no_grad():\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct_mask = pred.eq(target)\n",
        "            correct += correct_mask.sum().item()\n",
        "            total_samples += len(data)\n",
        "\n",
        "            confidence = F.softmax(output, dim=1).max(dim=1)[0]\n",
        "            high_conf_mask = confidence >= confidence_threshold\n",
        "            high_confidence_correct += (correct_mask & high_conf_mask).sum().item()\n",
        "            high_confidence_total += high_conf_mask.sum().item()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_confidence_loss += conf_loss.item()\n",
        "            total_ce_loss += ce_loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            accuracy = 100. * correct / total_samples\n",
        "            high_conf_acc = 100. * high_confidence_correct / max(1, high_confidence_total)\n",
        "\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] '\n",
        "                  f'Loss: {loss.item():.4f}, CE: {ce_loss.item():.4f}, Conf Loss: {conf_loss.item():.4f}, '\n",
        "                  f'Accuracy: {accuracy:.2f}%, High Conf Acc: {high_conf_acc:.2f}%, Time: {elapsed:.2f}s')\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_conf_loss = total_confidence_loss / len(train_loader)\n",
        "    avg_ce_loss = total_ce_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total_samples\n",
        "    high_conf_accuracy = 100. * high_confidence_correct / max(1, high_confidence_total)\n",
        "\n",
        "    print(f'Train Epoch {epoch} Summary: Loss: {avg_loss:.4f}, CE: {avg_ce_loss:.4f}, '\n",
        "          f'Conf Loss: {avg_conf_loss:.4f}, Accuracy: {accuracy:.2f}%, High Conf Acc: {high_conf_accuracy:.2f}%')\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'conf_loss': avg_conf_loss,\n",
        "        'ce_loss': avg_ce_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'high_conf_accuracy': high_conf_accuracy\n",
        "    }\n",
        "\n",
        "def test(model, device, test_loader, untrusted_loader, confidence_loss_fn, signatures, coverage_layer_indices, confidence_threshold=0.8):\n",
        "    model.eval()\n",
        "\n",
        "    def evaluate_loader(loader, is_trusted=True):\n",
        "        test_loss = confidence_loss = 0.0\n",
        "        correct = high_confidence_correct = total_samples = high_confidence_total = 0\n",
        "        confidence_scores = []\n",
        "        coverage_stats = [{\n",
        "            'within_range': 0,\n",
        "            'total': 0,\n",
        "            'coverage_percentage': 0,\n",
        "            'distribution_similarity': 0\n",
        "        } for _ in coverage_layer_indices]\n",
        "\n",
        "        confidence_bins = {\n",
        "            '0.0-0.2': {'correct': 0, 'total': 0},\n",
        "            '0.2-0.4': {'correct': 0, 'total': 0},\n",
        "            '0.4-0.6': {'correct': 0, 'total': 0},\n",
        "            '0.6-0.8': {'correct': 0, 'total': 0},\n",
        "            '0.8-0.9': {'correct': 0, 'total': 0},\n",
        "            '0.9-1.0': {'correct': 0, 'total': 0},\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                confidence = F.softmax(output, dim=1).max(dim=1)[0]\n",
        "                confidence_scores.extend(confidence.cpu().tolist())\n",
        "\n",
        "                if is_trusted:\n",
        "                    test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "                    conf_loss = confidence_loss_fn(model)\n",
        "                    confidence_loss += conf_loss.item()\n",
        "\n",
        "                    pred = output.argmax(dim=1)\n",
        "                    correct_mask = pred.eq(target)\n",
        "                    correct += correct_mask.sum().item()\n",
        "                    total_samples += len(data)\n",
        "\n",
        "                    high_conf_mask = confidence >= confidence_threshold\n",
        "                    high_confidence_correct += (correct_mask & high_conf_mask).sum().item()\n",
        "                    high_confidence_total += high_conf_mask.sum().item()\n",
        "\n",
        "                    # Update confidence bins\n",
        "                    for conf, corr in zip(confidence, correct_mask):\n",
        "                        conf_val = conf.item()\n",
        "                        if conf_val < 0.2:\n",
        "                            bin_key = '0.0-0.2'\n",
        "                        elif conf_val < 0.4:\n",
        "                            bin_key = '0.2-0.4'\n",
        "                        elif conf_val < 0.6:\n",
        "                            bin_key = '0.4-0.6'\n",
        "                        elif conf_val < 0.8:\n",
        "                            bin_key = '0.6-0.8'\n",
        "                        elif conf_val < 0.9:\n",
        "                            bin_key = '0.8-0.9'\n",
        "                        else:\n",
        "                            bin_key = '0.9-1.0'\n",
        "\n",
        "                        confidence_bins[bin_key]['total'] += 1\n",
        "                        if corr:\n",
        "                            confidence_bins[bin_key]['correct'] += 1\n",
        "\n",
        "                # Track coverage statistics\n",
        "                for stat_idx, layer_idx in enumerate(coverage_layer_indices):\n",
        "                    layer = model.coverage_layers[layer_idx]\n",
        "                    activations = layer.get_neuron_outputs()\n",
        "\n",
        "                    min_vals = signatures[layer_idx]['min'].to(device)\n",
        "                    max_vals = signatures[layer_idx]['max'].to(device)\n",
        "\n",
        "                    within_range = torch.logical_and(\n",
        "                        activations >= min_vals,\n",
        "                        activations <= max_vals\n",
        "                    ).float().sum().item()\n",
        "\n",
        "                    total_activations = activations.numel()\n",
        "                    coverage_stats[stat_idx]['within_range'] += within_range\n",
        "                    coverage_stats[stat_idx]['total'] += total_activations\n",
        "\n",
        "                    if signatures[layer_idx]['distributions'] is not None:\n",
        "                        num_sections = layer.num_sections\n",
        "                        delta = (max_vals - min_vals) / (num_sections - 1)\n",
        "                        delta = torch.where(delta == 0, torch.tensor(1e-10, device=delta.device), delta)\n",
        "\n",
        "                        section_indices = torch.clamp(((activations - min_vals) / delta).long(), 0, num_sections - 1)\n",
        "                        current_dist = torch.bincount(section_indices.view(-1), minlength=num_sections).float()\n",
        "                        current_dist /= activations.numel()\n",
        "\n",
        "                        signature_dist = signatures[layer_idx]['distributions'].to(device)\n",
        "                        dist_similarity = 1.0 - F.mse_loss(current_dist, signature_dist).item()\n",
        "                        coverage_stats[stat_idx]['distribution_similarity'] += dist_similarity\n",
        "\n",
        "        # Calculate final metrics\n",
        "        if is_trusted:\n",
        "            test_loss /= len(loader.dataset)\n",
        "            confidence_loss /= len(loader)\n",
        "            accuracy = 100. * correct / total_samples if total_samples > 0 else 0\n",
        "            high_conf_accuracy = 100. * high_confidence_correct / max(1, high_confidence_total)\n",
        "        else:\n",
        "            test_loss = confidence_loss = accuracy = high_conf_accuracy = 0\n",
        "\n",
        "        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0\n",
        "\n",
        "        # Calculate accuracy per confidence bin\n",
        "        bin_metrics = {}\n",
        "        for bin_key, values in confidence_bins.items():\n",
        "            if values['total'] > 0:\n",
        "                bin_acc = 100. * values['correct'] / values['total'] if is_trusted else 0\n",
        "                bin_metrics[bin_key] = {\n",
        "                    'accuracy': bin_acc,\n",
        "                    'samples': values['total'],\n",
        "                    'percentage': 100. * values['total'] / total_samples if total_samples > 0 else 0\n",
        "                }\n",
        "            else:\n",
        "                bin_metrics[bin_key] = {'accuracy': 0, 'samples': 0, 'percentage': 0}\n",
        "\n",
        "        # Calculate final coverage stats\n",
        "        for stat_idx in range(len(coverage_layer_indices)):\n",
        "            if coverage_stats[stat_idx]['total'] > 0:\n",
        "                coverage_stats[stat_idx]['coverage_percentage'] = (\n",
        "                    100.0 * coverage_stats[stat_idx]['within_range'] / coverage_stats[stat_idx]['total']\n",
        "                )\n",
        "\n",
        "                if signatures[coverage_layer_indices[stat_idx]]['distributions'] is not None:\n",
        "                    coverage_stats[stat_idx]['distribution_similarity'] /= len(loader)\n",
        "\n",
        "        return {\n",
        "            'loss': test_loss,\n",
        "            'conf_loss': confidence_loss,\n",
        "            'accuracy': accuracy,\n",
        "            'avg_confidence': avg_confidence,\n",
        "            'high_conf_accuracy': high_conf_accuracy,\n",
        "            'bin_metrics': bin_metrics,\n",
        "            'coverage_stats': coverage_stats,\n",
        "            'confidence_scores': confidence_scores\n",
        "        }\n",
        "\n",
        "    # Evaluate trusted data\n",
        "    print(\"\\nEvaluating on trusted data...\")\n",
        "    trusted_results = evaluate_loader(test_loader, is_trusted=True)\n",
        "\n",
        "    # Evaluate untrusted data\n",
        "    print(\"\\nEvaluating on untrusted data...\")\n",
        "    untrusted_results = evaluate_loader(untrusted_loader, is_trusted=False)\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\nConfidence Comparison:\")\n",
        "    print(f\"Trusted data avg confidence: {trusted_results['avg_confidence']:.4f}\")\n",
        "    print(f\"Untrusted data avg confidence: {untrusted_results['avg_confidence']:.4f}\")\n",
        "\n",
        "    print(\"\\nCoverage Comparison (trusted vs untrusted):\")\n",
        "    for i, layer_idx in enumerate(coverage_layer_indices[:5]):  # Show first 5 layers\n",
        "        trusted_cov = trusted_results['coverage_stats'][i]['coverage_percentage']\n",
        "        untrusted_cov = untrusted_results['coverage_stats'][i]['coverage_percentage']\n",
        "        print(f\"  Layer {layer_idx+1}: {trusted_cov:.2f}% vs {untrusted_cov:.2f}% coverage\")\n",
        "\n",
        "    return {\n",
        "        'trusted': trusted_results,\n",
        "        'untrusted': untrusted_results\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gnp_FIwJfOQ"
      },
      "source": [
        "# **test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCdvVtP_HiAQ"
      },
      "outputs": [],
      "source": [
        "def test(model, device, test_loader, untrusted_loader, confidence_loss_fn, signatures, coverage_layer_indices, confidence_threshold=0.8):\n",
        "    model.eval()\n",
        "\n",
        "    def evaluate_loader(loader, is_trusted=True):\n",
        "        test_loss = confidence_loss = 0.0\n",
        "        correct = high_confidence_correct = total_samples = high_confidence_total = 0\n",
        "        confidence_scores = []\n",
        "        coverage_stats = [{\n",
        "            'within_range': 0,\n",
        "            'total': 0,\n",
        "            'coverage_percentage': 0,\n",
        "            'distribution_similarity': 0\n",
        "        } for _ in coverage_layer_indices]\n",
        "\n",
        "        confidence_bins = {\n",
        "            '0.0-0.2': {'correct': 0, 'total': 0},\n",
        "            '0.2-0.4': {'correct': 0, 'total': 0},\n",
        "            '0.4-0.6': {'correct': 0, 'total': 0},\n",
        "            '0.6-0.8': {'correct': 0, 'total': 0},\n",
        "            '0.8-0.9': {'correct': 0, 'total': 0},\n",
        "            '0.9-1.0': {'correct': 0, 'total': 0},\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                confidence = F.softmax(output, dim=1).max(dim=1)[0]\n",
        "                confidence_scores.extend(confidence.cpu().tolist())\n",
        "\n",
        "                if is_trusted:\n",
        "                    test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "                    conf_loss = confidence_loss_fn(model)\n",
        "                    confidence_loss += conf_loss.item()\n",
        "\n",
        "                    pred = output.argmax(dim=1)\n",
        "                    correct_mask = pred.eq(target)\n",
        "                    correct += correct_mask.sum().item()\n",
        "                    total_samples += len(data)\n",
        "\n",
        "                    high_conf_mask = confidence >= confidence_threshold\n",
        "                    high_confidence_correct += (correct_mask & high_conf_mask).sum().item()\n",
        "                    high_confidence_total += high_conf_mask.sum().item()\n",
        "\n",
        "                    # Update confidence bins\n",
        "                    for conf, corr in zip(confidence, correct_mask):\n",
        "                        conf_val = conf.item()\n",
        "                        if conf_val < 0.2:\n",
        "                            bin_key = '0.0-0.2'\n",
        "                        elif conf_val < 0.4:\n",
        "                            bin_key = '0.2-0.4'\n",
        "                        elif conf_val < 0.6:\n",
        "                            bin_key = '0.4-0.6'\n",
        "                        elif conf_val < 0.8:\n",
        "                            bin_key = '0.6-0.8'\n",
        "                        elif conf_val < 0.9:\n",
        "                            bin_key = '0.8-0.9'\n",
        "                        else:\n",
        "                            bin_key = '0.9-1.0'\n",
        "\n",
        "                        confidence_bins[bin_key]['total'] += 1\n",
        "                        if corr:\n",
        "                            confidence_bins[bin_key]['correct'] += 1\n",
        "\n",
        "                # Track coverage statistics\n",
        "                for stat_idx, layer_idx in enumerate(coverage_layer_indices):\n",
        "                    layer = model.coverage_layers[layer_idx]\n",
        "                    activations = layer.get_neuron_outputs()\n",
        "\n",
        "                    min_vals = signatures[layer_idx]['min'].to(device)\n",
        "                    max_vals = signatures[layer_idx]['max'].to(device)\n",
        "\n",
        "                    within_range = torch.logical_and(\n",
        "                        activations >= min_vals,\n",
        "                        activations <= max_vals\n",
        "                    ).float().sum().item()\n",
        "\n",
        "                    total_activations = activations.numel()\n",
        "                    coverage_stats[stat_idx]['within_range'] += within_range\n",
        "                    coverage_stats[stat_idx]['total'] += total_activations\n",
        "\n",
        "                    if signatures[layer_idx]['distributions'] is not None:\n",
        "                        num_sections = layer.num_sections\n",
        "                        delta = (max_vals - min_vals) / (num_sections - 1)\n",
        "                        delta = torch.where(delta == 0, torch.tensor(1e-10, device=delta.device), delta)\n",
        "\n",
        "                        section_indices = torch.clamp(((activations - min_vals) / delta).long(), 0, num_sections - 1)\n",
        "                        current_dist = torch.bincount(section_indices.view(-1), minlength=num_sections).float()\n",
        "                        current_dist /= activations.numel()\n",
        "\n",
        "                        signature_dist = signatures[layer_idx]['distributions'].to(device)\n",
        "                        dist_similarity = 1.0 - F.mse_loss(current_dist, signature_dist).item()\n",
        "                        coverage_stats[stat_idx]['distribution_similarity'] += dist_similarity\n",
        "\n",
        "        # Calculate final metrics\n",
        "        if is_trusted:\n",
        "            test_loss /= len(loader.dataset)\n",
        "            confidence_loss /= len(loader)\n",
        "            accuracy = 100. * correct / total_samples if total_samples > 0 else 0\n",
        "            high_conf_accuracy = 100. * high_confidence_correct / max(1, high_confidence_total)\n",
        "        else:\n",
        "            test_loss = confidence_loss = accuracy = high_conf_accuracy = 0\n",
        "\n",
        "        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0\n",
        "\n",
        "        # Calculate accuracy per confidence bin\n",
        "        bin_metrics = {}\n",
        "        for bin_key, values in confidence_bins.items():\n",
        "            if values['total'] > 0:\n",
        "                bin_acc = 100. * values['correct'] / values['total'] if is_trusted else 0\n",
        "                bin_metrics[bin_key] = {\n",
        "                    'accuracy': bin_acc,\n",
        "                    'samples': values['total'],\n",
        "                    'percentage': 100. * values['total'] / total_samples if total_samples > 0 else 0\n",
        "                }\n",
        "            else:\n",
        "                bin_metrics[bin_key] = {'accuracy': 0, 'samples': 0, 'percentage': 0}\n",
        "\n",
        "        # Calculate final coverage stats\n",
        "        for stat_idx in range(len(coverage_layer_indices)):\n",
        "            if coverage_stats[stat_idx]['total'] > 0:\n",
        "                coverage_stats[stat_idx]['coverage_percentage'] = (\n",
        "                    100.0 * coverage_stats[stat_idx]['within_range'] / coverage_stats[stat_idx]['total']\n",
        "                )\n",
        "\n",
        "                if signatures[coverage_layer_indices[stat_idx]]['distributions'] is not None:\n",
        "                    coverage_stats[stat_idx]['distribution_similarity'] /= len(loader)\n",
        "\n",
        "        return {\n",
        "            'loss': test_loss,\n",
        "            'conf_loss': confidence_loss,\n",
        "            'accuracy': accuracy,\n",
        "            'avg_confidence': avg_confidence,\n",
        "            'high_conf_accuracy': high_conf_accuracy,\n",
        "            'bin_metrics': bin_metrics,\n",
        "            'coverage_stats': coverage_stats,\n",
        "            'confidence_scores': confidence_scores\n",
        "        }\n",
        "\n",
        "    # Evaluate trusted data\n",
        "    print(\"\\nEvaluating on trusted data...\")\n",
        "    trusted_results = evaluate_loader(test_loader, is_trusted=True)\n",
        "\n",
        "    # Evaluate untrusted data\n",
        "    print(\"\\nEvaluating on untrusted data...\")\n",
        "    untrusted_results = evaluate_loader(untrusted_loader, is_trusted=False)\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\nConfidence Comparison:\")\n",
        "    print(f\"Trusted data avg confidence: {trusted_results['avg_confidence']:.4f}\")\n",
        "    print(f\"Untrusted data avg confidence: {untrusted_results['avg_confidence']:.4f}\")\n",
        "\n",
        "    print(\"\\nCoverage Comparison (trusted vs untrusted):\")\n",
        "    for i, layer_idx in enumerate(coverage_layer_indices[:5]):  # Show first 5 layers\n",
        "        trusted_cov = trusted_results['coverage_stats'][i]['coverage_percentage']\n",
        "        untrusted_cov = untrusted_results['coverage_stats'][i]['coverage_percentage']\n",
        "        print(f\"  Layer {layer_idx+1}: {trusted_cov:.2f}% vs {untrusted_cov:.2f}% coverage\")\n",
        "\n",
        "    return {\n",
        "        'trusted': trusted_results,\n",
        "        'untrusted': untrusted_results\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfR-TrRZKBps"
      },
      "source": [
        "# **==================== MAIN FUNCTION ====================**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-FRnYRRKFf0"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfLMaxr5Hh95"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Configuration\n",
        "    batch_size = 64\n",
        "    epochs = 10\n",
        "    learning_rate = 0.001\n",
        "    momentum = 0.9\n",
        "    confidence_threshold = 0.9\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Data transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    mnist_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    untrusted_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Model setup\n",
        "    model = VGG19WithCoverage().to(device)\n",
        "    model.fc3 = nn.Linear(4096, 10).to(device)  # Modify for CIFAR-10\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "\n",
        "    # Coverage layer configuration\n",
        "    coverage_layer_indices = list(range(18))  # All 18 coverage layers\n",
        "\n",
        "    # Initialize data logger\n",
        "    logger = TrainingDataLogger()\n",
        "    logger.data['config'] = {\n",
        "        'batch_size': batch_size,\n",
        "        'epochs': epochs,\n",
        "        'learning_rate': learning_rate,\n",
        "        'confidence_threshold': confidence_threshold,\n",
        "        'model': 'VGG19WithCoverage'\n",
        "    }\n",
        "\n",
        "    # Initialize signature generator\n",
        "    signature_generator = SignatureGenerator(model, coverage_layer_indices, method='mrc')\n",
        "    signatures = signature_generator.aggregate_signatures(train_loader, device)\n",
        "\n",
        "    # Initialize confidence loss\n",
        "    confidence_loss_fn = ConfidenceLoss(signatures, coverage_layer_indices, method='mrc')\n",
        "\n",
        "    # Main training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"\\n{'='*20} Epoch {epoch}/{epochs} {'='*20}\")\n",
        "\n",
        "        # Update signatures every 5 epochs\n",
        "        if epoch % 5 == 0:\n",
        "            print(\"Updating activation signatures...\")\n",
        "            signatures = signature_generator.aggregate_signatures(train_loader, device)\n",
        "            confidence_loss_fn = ConfidenceLoss(signatures, coverage_layer_indices, method='mrc')\n",
        "\n",
        "        # Training\n",
        "        train_metrics = train(model, device, train_loader, optimizer, epoch,\n",
        "                            confidence_loss_fn, signatures, coverage_layer_indices,\n",
        "                            confidence_threshold)\n",
        "\n",
        "        # Testing\n",
        "        test_metrics = test(model, device, test_loader, untrusted_loader,\n",
        "                          confidence_loss_fn, signatures, coverage_layer_indices,\n",
        "                          confidence_threshold)\n",
        "\n",
        "        # Store all data\n",
        "        logger.add_epoch_data(epoch, train_metrics, test_metrics, test_metrics['trusted']['coverage_stats'])\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Epoch summary\n",
        "        print(f\"\\nEpoch {epoch} Summary:\")\n",
        "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, Accuracy: {train_metrics['accuracy']:.2f}%\")\n",
        "        print(f\"Test - Loss: {test_metrics['trusted']['loss']:.4f}, Accuracy: {test_metrics['trusted']['accuracy']:.2f}%\")\n",
        "        print(f\"Untrusted Data Avg Confidence: {test_metrics['untrusted']['avg_confidence']:.4f}\")\n",
        "\n",
        "    # Finalize and save data\n",
        "    logger.finalize(epochs)\n",
        "    save_filename = f\"training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    logger.save(save_filename)\n",
        "\n",
        "\n",
        "\n",
        "    # Final summary\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n{'='*20} Training Complete {'='*20}\")\n",
        "    print(f\"Total training time: {total_time:.2f}s ({total_time/60:.2f}min)\")\n",
        "    print(f\"Final test accuracy: {test_metrics['trusted']['accuracy']:.2f}%\")\n",
        "    print(f\"Final test avg confidence: {test_metrics['trusted']['avg_confidence']:.4f}\")\n",
        "    print(f\"Final high confidence accuracy: {test_metrics['trusted']['high_conf_accuracy']:.2f}%\")\n",
        "    print(f\"Final untrusted data avg confidence: {test_metrics['untrusted']['avg_confidence']:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHN1Icy1KQZF",
        "outputId": "712b8c56-d3fc-47eb-87c5-b6cd34c82600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.4MB/s]\n",
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 478kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.36MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.9MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:03<00:00, 172MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== Epoch 1/10 ====================\n",
            "Train Epoch: 1 [0/50000] Loss: 2.3256, CE: 2.3251, Conf Loss: 0.0005, Accuracy: 10.94%, High Conf Acc: 0.00%, Time: 2.78s\n",
            "Train Epoch: 1 [6400/50000] Loss: 0.6306, CE: 0.6043, Conf Loss: 0.0263, Accuracy: 60.32%, High Conf Acc: 94.80%, Time: 190.09s\n",
            "Train Epoch: 1 [12800/50000] Loss: 0.5326, CE: 0.5106, Conf Loss: 0.0221, Accuracy: 70.59%, High Conf Acc: 95.84%, Time: 377.57s\n",
            "Train Epoch: 1 [19200/50000] Loss: 0.5479, CE: 0.5238, Conf Loss: 0.0241, Accuracy: 75.37%, High Conf Acc: 96.53%, Time: 564.87s\n",
            "Train Epoch: 1 [25600/50000] Loss: 0.3856, CE: 0.3664, Conf Loss: 0.0192, Accuracy: 77.72%, High Conf Acc: 96.83%, Time: 752.31s\n",
            "Train Epoch: 1 [32000/50000] Loss: 0.4714, CE: 0.4548, Conf Loss: 0.0166, Accuracy: 79.43%, High Conf Acc: 97.08%, Time: 940.01s\n",
            "Train Epoch: 1 [38400/50000] Loss: 0.2430, CE: 0.2232, Conf Loss: 0.0198, Accuracy: 80.83%, High Conf Acc: 97.20%, Time: 1127.46s\n",
            "Train Epoch: 1 [44800/50000] Loss: 0.3485, CE: 0.3267, Conf Loss: 0.0219, Accuracy: 81.75%, High Conf Acc: 97.24%, Time: 1315.02s\n",
            "Train Epoch 1 Summary: Loss: 0.5266, CE: 0.5060, Conf Loss: 0.0206, Accuracy: 82.44%, High Conf Acc: 97.35%\n",
            "\n",
            "Evaluating on trusted data...\n",
            "\n",
            "Evaluating on untrusted data...\n",
            "\n",
            "Confidence Comparison:\n",
            "Trusted data avg confidence: 0.8968\n",
            "Untrusted data avg confidence: 0.5868\n",
            "\n",
            "Coverage Comparison (trusted vs untrusted):\n",
            "  Layer 1: 99.98% vs 97.79% coverage\n",
            "  Layer 2: 99.99% vs 99.16% coverage\n",
            "  Layer 3: 100.00% vs 99.88% coverage\n",
            "  Layer 4: 100.00% vs 99.88% coverage\n",
            "  Layer 5: 100.00% vs 99.92% coverage\n",
            "\n",
            "Epoch 1 Summary:\n",
            "Train - Loss: 0.5266, Accuracy: 82.44%\n",
            "Test - Loss: 0.3018, Accuracy: 89.82%\n",
            "Untrusted Data Avg Confidence: 0.5868\n",
            "\n",
            "==================== Epoch 2/10 ====================\n",
            "Train Epoch: 2 [0/50000] Loss: 0.2252, CE: 0.2059, Conf Loss: 0.0193, Accuracy: 92.19%, High Conf Acc: 100.00%, Time: 2.52s\n",
            "Train Epoch: 2 [6400/50000] Loss: 0.3872, CE: 0.3639, Conf Loss: 0.0233, Accuracy: 89.46%, High Conf Acc: 98.18%, Time: 190.10s\n",
            "Train Epoch: 2 [12800/50000] Loss: 0.3433, CE: 0.3188, Conf Loss: 0.0245, Accuracy: 89.86%, High Conf Acc: 98.19%, Time: 377.48s\n",
            "Train Epoch: 2 [19200/50000] Loss: 0.4405, CE: 0.4141, Conf Loss: 0.0264, Accuracy: 90.03%, High Conf Acc: 98.23%, Time: 565.25s\n",
            "Train Epoch: 2 [25600/50000] Loss: 0.2481, CE: 0.2278, Conf Loss: 0.0203, Accuracy: 90.25%, High Conf Acc: 98.21%, Time: 752.87s\n",
            "Train Epoch: 2 [32000/50000] Loss: 0.1849, CE: 0.1605, Conf Loss: 0.0244, Accuracy: 90.27%, High Conf Acc: 98.24%, Time: 940.51s\n",
            "Train Epoch: 2 [38400/50000] Loss: 0.2485, CE: 0.2269, Conf Loss: 0.0215, Accuracy: 90.32%, High Conf Acc: 98.25%, Time: 1128.02s\n",
            "Train Epoch: 2 [44800/50000] Loss: 0.2922, CE: 0.2740, Conf Loss: 0.0182, Accuracy: 90.36%, High Conf Acc: 98.30%, Time: 1315.35s\n",
            "Train Epoch 2 Summary: Loss: 0.2990, CE: 0.2764, Conf Loss: 0.0227, Accuracy: 90.48%, High Conf Acc: 98.29%\n",
            "\n",
            "Evaluating on trusted data...\n",
            "\n",
            "Evaluating on untrusted data...\n",
            "\n",
            "Confidence Comparison:\n",
            "Trusted data avg confidence: 0.9197\n",
            "Untrusted data avg confidence: 0.6466\n",
            "\n",
            "Coverage Comparison (trusted vs untrusted):\n",
            "  Layer 1: 99.94% vs 97.43% coverage\n",
            "  Layer 2: 99.99% vs 98.85% coverage\n",
            "  Layer 3: 100.00% vs 99.84% coverage\n",
            "  Layer 4: 100.00% vs 99.86% coverage\n",
            "  Layer 5: 100.00% vs 99.91% coverage\n",
            "\n",
            "Epoch 2 Summary:\n",
            "Train - Loss: 0.2990, Accuracy: 90.48%\n",
            "Test - Loss: 0.2401, Accuracy: 91.84%\n",
            "Untrusted Data Avg Confidence: 0.6466\n",
            "\n",
            "==================== Epoch 3/10 ====================\n",
            "Train Epoch: 3 [0/50000] Loss: 0.0770, CE: 0.0543, Conf Loss: 0.0227, Accuracy: 98.44%, High Conf Acc: 100.00%, Time: 2.51s\n",
            "Train Epoch: 3 [6400/50000] Loss: 0.4878, CE: 0.4566, Conf Loss: 0.0312, Accuracy: 93.15%, High Conf Acc: 98.85%, Time: 189.91s\n",
            "Train Epoch: 3 [12800/50000] Loss: 0.1236, CE: 0.0966, Conf Loss: 0.0270, Accuracy: 92.81%, High Conf Acc: 98.62%, Time: 377.38s\n",
            "Train Epoch: 3 [19200/50000] Loss: 0.3422, CE: 0.3193, Conf Loss: 0.0229, Accuracy: 92.61%, High Conf Acc: 98.56%, Time: 564.93s\n",
            "Train Epoch: 3 [25600/50000] Loss: 0.1355, CE: 0.1115, Conf Loss: 0.0239, Accuracy: 92.47%, High Conf Acc: 98.61%, Time: 752.53s\n",
            "Train Epoch: 3 [32000/50000] Loss: 0.3361, CE: 0.3051, Conf Loss: 0.0311, Accuracy: 92.52%, High Conf Acc: 98.63%, Time: 940.05s\n",
            "Train Epoch: 3 [38400/50000] Loss: 0.1825, CE: 0.1599, Conf Loss: 0.0226, Accuracy: 92.47%, High Conf Acc: 98.64%, Time: 1127.60s\n",
            "Train Epoch: 3 [44800/50000] Loss: 0.2217, CE: 0.1919, Conf Loss: 0.0298, Accuracy: 92.39%, High Conf Acc: 98.64%, Time: 1315.03s\n",
            "Train Epoch 3 Summary: Loss: 0.2467, CE: 0.2214, Conf Loss: 0.0252, Accuracy: 92.34%, High Conf Acc: 98.66%\n",
            "\n",
            "Evaluating on trusted data...\n",
            "\n",
            "Evaluating on untrusted data...\n",
            "\n",
            "Confidence Comparison:\n",
            "Trusted data avg confidence: 0.9219\n",
            "Untrusted data avg confidence: 0.6385\n",
            "\n",
            "Coverage Comparison (trusted vs untrusted):\n",
            "  Layer 1: 99.93% vs 97.75% coverage\n",
            "  Layer 2: 99.98% vs 98.86% coverage\n",
            "  Layer 3: 100.00% vs 99.85% coverage\n",
            "  Layer 4: 99.99% vs 99.86% coverage\n",
            "  Layer 5: 99.99% vs 99.90% coverage\n",
            "\n",
            "Epoch 3 Summary:\n",
            "Train - Loss: 0.2467, Accuracy: 92.34%\n",
            "Test - Loss: 0.2340, Accuracy: 92.10%\n",
            "Untrusted Data Avg Confidence: 0.6385\n",
            "\n",
            "==================== Epoch 4/10 ====================\n",
            "Train Epoch: 4 [0/50000] Loss: 0.1183, CE: 0.0959, Conf Loss: 0.0224, Accuracy: 96.88%, High Conf Acc: 100.00%, Time: 2.42s\n",
            "Train Epoch: 4 [6400/50000] Loss: 0.2558, CE: 0.2287, Conf Loss: 0.0272, Accuracy: 94.06%, High Conf Acc: 98.97%, Time: 190.01s\n",
            "Train Epoch: 4 [12800/50000] Loss: 0.1923, CE: 0.1660, Conf Loss: 0.0263, Accuracy: 93.87%, High Conf Acc: 98.80%, Time: 377.64s\n",
            "Train Epoch: 4 [19200/50000] Loss: 0.3315, CE: 0.3059, Conf Loss: 0.0257, Accuracy: 93.84%, High Conf Acc: 98.84%, Time: 565.09s\n",
            "Train Epoch: 4 [25600/50000] Loss: 0.1817, CE: 0.1511, Conf Loss: 0.0306, Accuracy: 93.83%, High Conf Acc: 98.87%, Time: 751.86s\n",
            "Train Epoch: 4 [32000/50000] Loss: 0.1531, CE: 0.1275, Conf Loss: 0.0256, Accuracy: 93.78%, High Conf Acc: 98.86%, Time: 939.20s\n",
            "Train Epoch: 4 [38400/50000] Loss: 0.2317, CE: 0.1985, Conf Loss: 0.0332, Accuracy: 93.80%, High Conf Acc: 98.90%, Time: 1126.59s\n",
            "Train Epoch: 4 [44800/50000] Loss: 0.2023, CE: 0.1758, Conf Loss: 0.0265, Accuracy: 93.83%, High Conf Acc: 98.87%, Time: 1313.83s\n",
            "Train Epoch 4 Summary: Loss: 0.2086, CE: 0.1798, Conf Loss: 0.0288, Accuracy: 93.81%, High Conf Acc: 98.87%\n",
            "\n",
            "Evaluating on trusted data...\n",
            "\n",
            "Evaluating on untrusted data...\n",
            "\n",
            "Confidence Comparison:\n",
            "Trusted data avg confidence: 0.9337\n",
            "Untrusted data avg confidence: 0.6538\n",
            "\n",
            "Coverage Comparison (trusted vs untrusted):\n",
            "  Layer 1: 99.95% vs 96.61% coverage\n",
            "  Layer 2: 99.99% vs 98.70% coverage\n",
            "  Layer 3: 100.00% vs 99.83% coverage\n",
            "  Layer 4: 99.99% vs 99.85% coverage\n",
            "  Layer 5: 99.99% vs 99.88% coverage\n",
            "\n",
            "Epoch 4 Summary:\n",
            "Train - Loss: 0.2086, Accuracy: 93.81%\n",
            "Test - Loss: 0.2148, Accuracy: 92.69%\n",
            "Untrusted Data Avg Confidence: 0.6538\n",
            "\n",
            "==================== Epoch 5/10 ====================\n",
            "Updating activation signatures...\n",
            "Train Epoch: 5 [0/50000] Loss: 0.1623, CE: 0.1617, Conf Loss: 0.0005, Accuracy: 95.31%, High Conf Acc: 100.00%, Time: 2.92s\n",
            "Train Epoch: 5 [6400/50000] Loss: 0.0708, CE: 0.0696, Conf Loss: 0.0013, Accuracy: 94.85%, High Conf Acc: 99.12%, Time: 189.96s\n",
            "Train Epoch: 5 [12800/50000] Loss: 0.0952, CE: 0.0929, Conf Loss: 0.0022, Accuracy: 94.66%, High Conf Acc: 99.06%, Time: 376.49s\n",
            "Train Epoch: 5 [19200/50000] Loss: 0.1161, CE: 0.1146, Conf Loss: 0.0015, Accuracy: 94.58%, High Conf Acc: 99.09%, Time: 562.78s\n",
            "Train Epoch: 5 [25600/50000] Loss: 0.3250, CE: 0.3237, Conf Loss: 0.0013, Accuracy: 94.49%, High Conf Acc: 99.01%, Time: 748.80s\n",
            "Train Epoch: 5 [32000/50000] Loss: 0.2838, CE: 0.2824, Conf Loss: 0.0014, Accuracy: 94.60%, High Conf Acc: 99.04%, Time: 934.79s\n",
            "Train Epoch: 5 [38400/50000] Loss: 0.1687, CE: 0.1672, Conf Loss: 0.0015, Accuracy: 94.69%, High Conf Acc: 99.03%, Time: 1120.70s\n",
            "Train Epoch: 5 [44800/50000] Loss: 0.1413, CE: 0.1387, Conf Loss: 0.0026, Accuracy: 94.67%, High Conf Acc: 99.04%, Time: 1306.25s\n",
            "Train Epoch 5 Summary: Loss: 0.1571, CE: 0.1557, Conf Loss: 0.0014, Accuracy: 94.68%, High Conf Acc: 99.04%\n",
            "\n",
            "Evaluating on trusted data...\n",
            "\n",
            "Evaluating on untrusted data...\n",
            "\n",
            "Confidence Comparison:\n",
            "Trusted data avg confidence: 0.9454\n",
            "Untrusted data avg confidence: 0.6624\n",
            "\n",
            "Coverage Comparison (trusted vs untrusted):\n",
            "  Layer 1: 99.97% vs 98.47% coverage\n",
            "  Layer 2: 99.99% vs 99.72% coverage\n",
            "  Layer 3: 100.00% vs 99.85% coverage\n",
            "  Layer 4: 100.00% vs 99.84% coverage\n",
            "  Layer 5: 100.00% vs 99.90% coverage\n",
            "\n",
            "Epoch 5 Summary:\n",
            "Train - Loss: 0.1571, Accuracy: 94.68%\n",
            "Test - Loss: 0.2108, Accuracy: 92.88%\n",
            "Untrusted Data Avg Confidence: 0.6624\n",
            "\n",
            "==================== Epoch 6/10 ====================\n",
            "Train Epoch: 6 [0/50000] Loss: 0.0684, CE: 0.0668, Conf Loss: 0.0016, Accuracy: 98.44%, High Conf Acc: 100.00%, Time: 2.47s\n",
            "Train Epoch: 6 [6400/50000] Loss: 0.1003, CE: 0.0974, Conf Loss: 0.0029, Accuracy: 95.58%, High Conf Acc: 99.23%, Time: 188.14s\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}